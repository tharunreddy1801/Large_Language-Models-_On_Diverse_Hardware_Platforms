# Large_Language-Models-_On__Diverse_Hardware_Platforms
I have deployed Existing Hugging Face model on Diverse Hardware platforms to reduce Latency, and increase throughput. FPGA works on Parallelism in which it's computational Power will be reduce by deploying FPGA
## üîç Problem Statement
With the exponential growth of LLMs, efficient deployment across diverse hardware platforms has become crucial for:
* **Scalability** across different computational environments
* **Cost optimization** for enterprise deployments
* **Energy efficiency** for sustainable AI practices
* **Real-time applications** requiring low-latency inference
## ‚ú® Key Features
* üèóÔ∏è **Multi-Hardware Support**: CPU, GPU, and FPGA implementations
* üîÑ **Model Conversion Pipeline**: Hugging Face ‚Üí ONNX ‚Üí Hardware-specific optimization
* ‚ö° **Performance Benchmarking**: Comprehensive latency and throughput analysis
* üìä **Visualization Tools**: Real-time performance monitoring and comparison
* üéØ **Model Optimization**: Quantization and compression techniques
* üåê **Framework Agnostic**: Support for multiple ML frameworks give this lines in readme code
 ##  üìÅ Directory Structure
* ‚îú‚îÄ‚îÄ üìÅ src/                                                  # Source code files
* ‚îú‚îÄ‚îÄ üìÅ BroadRange_FPGA/                                      # work_experience
* ‚îú‚îÄ‚îÄ üìÅ Models/                                               # Models
* ‚îú‚îÄ‚îÄ üìÑ onnx and HuggingFace.ipynb                            # code
* ‚îú‚îÄ‚îÄ üìÑ graphs.docx                                           # Visualisation
* ‚îú‚îÄ‚îÄ üìÑ llm on Diverse hardware platforms_report.pdf          # report
* ‚îú‚îÄ‚îÄ üìÑ README.md               # Project overview and setup
* ‚îî‚îÄ‚îÄ üìÑ LICENSE                 # License file
## üèõÔ∏è Architecture
  * A[Hugging Face Model] --> B[ONNX Conversion]
  * B --> C[Hardware Optimization]
  * C --> D[CPU Deployment]
  * C --> E[GPU Deployment] 
  * C --> F[FPGA Deployment]
  * D,E,F --> G[Performance Analysis]
## Core Components

* **Model Layer**: Pre-trained transformers (BERT, TinyBERT)
* **Conversion Layer**: ONNX standardization for cross-platform compatibility
* **Hardware Layer**: Platform-specific optimizations
* **Analysis Layer**: Performance monitoring and visualization
## üõ†Ô∏è Tech Stack
***Core Technologies***

* **Python 3.8+**: Primary programming language
* **PyTorch**: Deep learning framework
* **ONNX**: Cross-platform model standardization
* **Hugging Face Transformers**: Pre-trained model library

***Hardware Platforms***

* **CPU**: Intel/AMD x86_64 architectures
* **GPU**: NVIDIA CUDA-enabled GPUs
* **FPGA**: Xilinx/Intel FPGA platforms (planned)

***Development Tools***

* **Jupyter Notebooks**: Interactive development
* **Matplotlib/Seaborn**: Data visualization
* **Netron**: Neural network visualization
* **Git**: Version control
## ‚öôÔ∏è Setup Instructions
### 1. Clone the Repository
```bash
git clone https://github.com/tharunreddy1801/Large_Language-Models-_On_Diverse_Hardware_Platforms.git
cd Large_Language-Models-_On_Diverse_Hardware_Platforms
```
### 2. Install Requirements
```bash
pip install -r requirements.txt
```
If `requirements.txt` is missing, use:
```bash
pip install torch transformers onnx onnxruntime huggingface-hub numpy pandas matplotlib seaborn jupyter notebook
```
### 3. Download & Organize Models
Download models from Hugging Face and place them like:
```bash
models/
‚îú‚îÄ‚îÄ bert-base-uncased/
‚îú‚îÄ‚îÄ bert-large-uncased/
‚îú‚îÄ‚îÄ distilbert-base-uncased/
‚îî‚îÄ‚îÄ huawei-noah--TinyBERT_General_4L_312D/
```
### 4. Setup Hardware Platforms
For CPU Platform:
```bash
pip install onnx
pip install onnxruntime
```
For GPU(Paper Space[Vm]) Platform:
```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install onnxruntime-gpu
```
For FPGA Platform:
```bash
pip install pynq
```
### 5. Verify Installation
```bash
python scripts/verify_setup.py
```
### 6. Convert Models to ONNX
```bash
python convert_models.py --model bert-base-uncased --output models/onnx/
```
### 7. Run Performance Benchmarks
```bash
python benchmark.py --platforms cpu,gpu,fpga --models all
```
### 8. Start Jupyter Notebook (Optional)
```bash
jupyter notebook
```
### 9. Quick Test Run
```bash
python quick_test.py --model bert-base-uncased --platform cpu
```
## üìà Performance Results
* Benchmark Summary (Input: "What is capital of India?")
![image](https://github.com/user-attachments/assets/2f1baff8-8745-446d-be39-879d13f9b70e)

* üéâ **Key Achievement**: ONNX optimization provides up to 9.1x speedup on GPU and 5.8x speedup on CPU
## Energy Efficiency Metrics
![image](https://github.com/user-attachments/assets/249f04e8-4c4c-433b-81ad-b2199cb24fb2)
* Estimated based on research analysis
## üîÆ Future Scope

 * **FPGA Implementation**: Complete FPGA deployment and optimization
 * **Quantum Computing**: Explore quantum-classical hybrid approaches
 * **Edge Computing**: Mobile and IoT device optimization
 * **AutoML Integration**: Automated hardware-software co-design
 * **Real-time Applications**: Chatbot and voice assistant implementations
 * **Security Features**: Privacy-preserving LLM deployment
## üôè Acknowledgments

* Hugging Face for pre-trained models
* ONNX community for optimization tools
* Xilinx  for FPGA development tools
* PyTorch and TensorFlow communities

## üìö References

* ONNX Documentation
* Hugging Face Transformers
* FlightLLMs
* Hardware-Aware Model Optimization
## üë®‚Äçüíª Author
Y.Sai Tharun Reddy

